{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pomegranate as pm\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate.distributions import Uniform, Normal\n",
    "from pomegranate.hmm import DenseHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "# logging.captureWarnings(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.params import get_params\n",
    "from scripts.aux_functions import generate_columns, save_as_pickle, get_all_results_matching, clean_modelname\n",
    "\n",
    "params = get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroute = params[\"dataroute\"]\n",
    "resultsroute = params[\"resultsroute\"]\n",
    "dumproute = params[\"dumproute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'finaldf_train_{params[\"tablename\"]}.pickle'\n",
    "filename = os.path.join(dataroute, name)\n",
    "with open(filename, \"rb\") as handle:\n",
    "    df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^BVSP_rets</th>\n",
       "      <th>^BVSP_log_rets</th>\n",
       "      <th>^BVSP_gk_vol</th>\n",
       "      <th>VALE3.SA_rets</th>\n",
       "      <th>VALE3.SA_log_rets</th>\n",
       "      <th>VALE3.SA_gk_vol</th>\n",
       "      <th>VALE_rets</th>\n",
       "      <th>VALE_log_rets</th>\n",
       "      <th>VALE_gk_vol</th>\n",
       "      <th>PETR3.SA_rets</th>\n",
       "      <th>...</th>\n",
       "      <th>ABEV3.SA_gk_vol</th>\n",
       "      <th>ABEV_rets</th>\n",
       "      <th>ABEV_log_rets</th>\n",
       "      <th>ABEV_gk_vol</th>\n",
       "      <th>USD_rets</th>\n",
       "      <th>USD_log_rets</th>\n",
       "      <th>USD_gk_vol</th>\n",
       "      <th>^BVSP_USD_rets</th>\n",
       "      <th>^BVSP_USD_log_rets</th>\n",
       "      <th>^BVSP_USD_gk_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>0.012182</td>\n",
       "      <td>0.012109</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>-0.017007</td>\n",
       "      <td>-0.017153</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.011168</td>\n",
       "      <td>-0.011231</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.037298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.006896</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>-0.012462</td>\n",
       "      <td>-0.012540</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.015455</td>\n",
       "      <td>-0.015576</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.008471</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.009110</td>\n",
       "      <td>-0.009152</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>-0.012968</td>\n",
       "      <td>-0.013053</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>-0.009437</td>\n",
       "      <td>-0.009481</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>-0.019681</td>\n",
       "      <td>-0.019878</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>-0.018510</td>\n",
       "      <td>-0.018683</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.013075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.007814</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.004489</td>\n",
       "      <td>-0.004499</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>-0.012998</td>\n",
       "      <td>-0.013083</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>-0.007887</td>\n",
       "      <td>-0.007919</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>-0.014990</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.028460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.017548</td>\n",
       "      <td>-0.017704</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-09</th>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ^BVSP_rets  ^BVSP_log_rets  ^BVSP_gk_vol  VALE3.SA_rets  \\\n",
       "2013-01-03    0.012182        0.012109      0.000218      -0.017007   \n",
       "2013-01-04   -0.012462       -0.012540      0.000163      -0.015455   \n",
       "2013-01-07   -0.009437       -0.009481      0.000180      -0.019681   \n",
       "2013-01-08   -0.012998       -0.013083      0.000250      -0.007887   \n",
       "2013-01-09    0.007378        0.007351      0.000087       0.004577   \n",
       "\n",
       "            VALE3.SA_log_rets  VALE3.SA_gk_vol  VALE_rets  VALE_log_rets  \\\n",
       "2013-01-03          -0.017153         0.000190  -0.011168      -0.011231   \n",
       "2013-01-04          -0.015576         0.000512  -0.008471      -0.008507   \n",
       "2013-01-07          -0.019878         0.000541  -0.018510      -0.018683   \n",
       "2013-01-08          -0.007919         0.000184  -0.014990      -0.015104   \n",
       "2013-01-09           0.004567         0.000137   0.001964       0.001962   \n",
       "\n",
       "            VALE_gk_vol  PETR3.SA_rets  ...  ABEV3.SA_gk_vol  ABEV_rets  \\\n",
       "2013-01-03     0.000204       0.037298  ...         0.000185   0.006920   \n",
       "2013-01-04     0.000265       0.003401  ...         0.000270   0.000711   \n",
       "2013-01-07     0.000324      -0.013075  ...         0.000146  -0.007814   \n",
       "2013-01-08     0.000108      -0.028460  ...         0.000141   0.005967   \n",
       "2013-01-09     0.000136       0.010101  ...         0.000309   0.007117   \n",
       "\n",
       "            ABEV_log_rets  ABEV_gk_vol  USD_rets  USD_log_rets  USD_gk_vol  \\\n",
       "2013-01-03       0.006896     0.000123  0.005423      0.005409    0.000005   \n",
       "2013-01-04       0.000711     0.000056 -0.009110     -0.009152    0.000127   \n",
       "2013-01-07      -0.007845     0.000065  0.002544      0.002541    0.000056   \n",
       "2013-01-08       0.005949     0.000061  0.002794      0.002790    0.000030   \n",
       "2013-01-09       0.007092     0.000037  0.003096      0.003092    0.000028   \n",
       "\n",
       "            ^BVSP_USD_rets  ^BVSP_USD_log_rets  ^BVSP_USD_gk_vol  \n",
       "2013-01-03        0.008609            0.008572          0.000218  \n",
       "2013-01-04       -0.012968           -0.013053          0.000163  \n",
       "2013-01-07       -0.004489           -0.004499          0.000180  \n",
       "2013-01-08       -0.017548           -0.017704          0.000250  \n",
       "2013-01-09        0.009302            0.009259          0.000087  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_states = range(1, 16)\n",
    "emptydf = pd.DataFrame(columns=[\"AIC\", \"BIC\"], index=range_states)\n",
    "emptydf.fillna(np.inf, inplace=True)\n",
    "results_dict_df = {stock: emptydf for stock in params[\"tickerlist\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array([[df[cols].values]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Normal in module pomegranate.distributions.normal:\n",
      "\n",
      "class Normal(pomegranate.distributions._distribution.Distribution)\n",
      " |  Normal(means=None, covs=None, covariance_type='full', min_cov=None, inertia=0.0, frozen=False, check_data=True)\n",
      " |  \n",
      " |  A normal distribution object.\n",
      " |  \n",
      " |  A normal distribution models the probability of a variable occurring under\n",
      " |  a bell-shaped curve. It is described by a vector of mean values and a\n",
      " |  covariance value that can be zero, one, or two dimensional. This\n",
      " |  distribution can assume that features are independent of the others if\n",
      " |  the covariance type is 'diag' or 'sphere', but if the type is 'full' then\n",
      " |  the features are not independent.\n",
      " |  \n",
      " |  There are two ways to initialize this object. The first is to pass in\n",
      " |  the tensor of probability parameters, at which point they can immediately be\n",
      " |  used. The second is to not pass in the rate parameters and then call\n",
      " |  either `fit` or `summary` + `from_summaries`, at which point the probability\n",
      " |  parameter will be learned from data.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  means: list, numpy.ndarray, torch.Tensor or None, shape=(d,), optional\n",
      " |          The mean values of the distributions. Default is None.\n",
      " |  \n",
      " |  covs: list, numpy.ndarray, torch.Tensor, or None, optional\n",
      " |          The variances and covariances of the distribution. If covariance_type\n",
      " |          is 'full', the shape should be (self.d, self.d); if 'diag', the shape\n",
      " |          should be (self.d,); if 'sphere', it should be (1,). Note that this is\n",
      " |          the variances or covariances in all settings, and not the standard\n",
      " |          deviation, as may be more common for diagonal covariance matrices.\n",
      " |          Default is None.\n",
      " |  \n",
      " |  covariance_type: str, optional\n",
      " |          The type of covariance matrix. Must be one of 'full', 'diag', or\n",
      " |          'sphere'. Default is 'full'. \n",
      " |  \n",
      " |  min_cov: float or None, optional\n",
      " |          The minimum variance or covariance.\n",
      " |  \n",
      " |  inertia: float, [0, 1], optional\n",
      " |          Indicates the proportion of the update to apply to the parameters\n",
      " |          during training. When the inertia is 0.0, the update is applied in\n",
      " |          its entirety and the previous parameters are ignored. When the\n",
      " |          inertia is 1.0, the update is entirely ignored and the previous\n",
      " |          parameters are kept, equivalently to if the parameters were frozen.\n",
      " |  \n",
      " |  frozen: bool, optional\n",
      " |          Whether all the parameters associated with this distribution are frozen.\n",
      " |          If you want to freeze individual pameters, or individual values in those\n",
      " |          parameters, you must modify the `frozen` attribute of the tensor or\n",
      " |          parameter directly. Default is False.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Normal\n",
      " |      pomegranate.distributions._distribution.Distribution\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, means=None, covs=None, covariance_type='full', min_cov=None, inertia=0.0, frozen=False, check_data=True)\n",
      " |      Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  from_summaries(self)\n",
      " |      Update the model parameters given the extracted statistics.\n",
      " |      \n",
      " |      This method uses calculated statistics from calls to the `summarize`\n",
      " |      method to update the distribution parameters. Hyperparameters for the\n",
      " |      update are passed in at initialization time.\n",
      " |      \n",
      " |      Note: Internally, a call to `fit` is just a successive call to the\n",
      " |      `summarize` method followed by the `from_summaries` method.\n",
      " |  \n",
      " |  log_probability(self, X)\n",
      " |      Calculate the log probability of each example.\n",
      " |      \n",
      " |      This method calculates the log probability of each example given the\n",
      " |      parameters of the distribution. The examples must be given in a 2D\n",
      " |      format.\n",
      " |      \n",
      " |      Note: This differs from some other log probability calculation\n",
      " |      functions, like those in torch.distributions, because it is not\n",
      " |      returning the log probability of each feature independently, but rather\n",
      " |      the total log probability of the entire example.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: list, tuple, numpy.ndarray, torch.Tensor, shape=(-1, self.d)\n",
      " |              A set of examples to evaluate.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logp: torch.Tensor, shape=(-1,)\n",
      " |              The log probability of each example.\n",
      " |  \n",
      " |  sample(self, n)\n",
      " |      Sample from the probability distribution.\n",
      " |      \n",
      " |      This method will return `n` samples generated from the underlying\n",
      " |      probability distribution.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n: int\n",
      " |              The number of samples to generate.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X: torch.tensor, shape=(n, self.d)\n",
      " |              Randomly generated samples.\n",
      " |  \n",
      " |  summarize(self, X, sample_weight=None)\n",
      " |      Extract the sufficient statistics from a batch of data.\n",
      " |      \n",
      " |      This method calculates the sufficient statistics from optionally\n",
      " |      weighted data and adds them to the stored cache. The examples must be\n",
      " |      given in a 2D format. Sample weights can either be provided as one\n",
      " |      value per example or as a 2D matrix of weights for each feature in\n",
      " |      each example.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: list, tuple, numpy.ndarray, torch.Tensor, shape=(-1, self.d)\n",
      " |              A set of examples to summarize.\n",
      " |      \n",
      " |      sample_weight: list, tuple, numpy.ndarray, torch.Tensor, optional\n",
      " |              A set of weights for the examples. This can be either of shape\n",
      " |              (-1, self.d) or a vector of shape (-1,). Default is ones.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pomegranate.distributions._distribution.Distribution:\n",
      " |  \n",
      " |  backward(self, X)\n",
      " |  \n",
      " |  fit(self, X, sample_weight=None)\n",
      " |  \n",
      " |  forward(self, X)\n",
      " |      Define the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  probability(self, X)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pomegranate.distributions._distribution.Distribution:\n",
      " |  \n",
      " |  device\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |      \n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Set the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module.\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |      \n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |      \n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): whether to assign items in the state\n",
      " |              dictionary to their corresponding keys in the module instead\n",
      " |              of copying them inplace into the module's current parameters and buffers.\n",
      " |              When ``False``, the properties of the tensors in the current\n",
      " |              module are preserved while when ``True``, the properties of the\n",
      " |              Tensors in the state dict are preserved.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post hook to be run after module's ``load_state_dict`` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n",
      " |      \n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |      \n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Set the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |      \n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2578, 2)\n"
     ]
    }
   ],
   "source": [
    "num_states=2\n",
    "\n",
    "n1 = Normal(means=[0])\n",
    "\n",
    "\n",
    "model = DenseHMM()\n",
    "cols=[\"VALE3.SA_log_rets\",\"VALE3.SA_gk_vol\"]\n",
    "\n",
    "model.add_distributions([n1 for _ in range(num_states*len(cols))])\n",
    "#X = torch.randn(100, 50, 2) #\n",
    "X= np.array([[df[cols].values]])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2578, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reshaped = data[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.0171533 ],\n",
       "        [ 0.00018979]],\n",
       "\n",
       "       [[-0.01557612],\n",
       "        [ 0.00051209]],\n",
       "\n",
       "       [[-0.01987763],\n",
       "        [ 0.00054097]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00756658],\n",
       "        [ 0.00063023]],\n",
       "\n",
       "       [[-0.0238272 ],\n",
       "        [ 0.00108552]],\n",
       "\n",
       "       [[-0.0073386 ],\n",
       "        [ 0.00024527]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = torch.randn(10, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseHMM(\n",
       "  (start): Silent()\n",
       "  (end): Silent()\n",
       "  (distributions): ModuleList(\n",
       "    (0-1): 2 x Normal()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = DenseHMM([Normal(), Normal()], sample_length=1)\n",
    "model3.fit(data_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model3.predict(data_reshaped).sum(axis=1)==2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states=5\n",
    "\n",
    "model = DenseHMM(distributions=[Normal() for _ in range(num_states)])\n",
    "\n",
    "data=df[cols].values\n",
    "data_reshaped=data[:, :, np.newaxis]\n",
    "res=model.fit(data_reshaped)\n",
    "prediction=res.predict(data_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        ...,\n",
       "        [2, 0],\n",
       "        [3, 4],\n",
       "        [2, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimal_states(data, max_states=10):\n",
    "  \"\"\"\n",
    "  Selects the optimal number of states for an HMM model using AIC.\n",
    "\n",
    "  Args:\n",
    "      data (list): List of observation sequences.\n",
    "      max_states (int, optional): Upper limit for the number of states to explore. Defaults to 10.\n",
    "\n",
    "  Returns:\n",
    "      tuple: (number_of_states, model) - The optimal number of states and the fitted model.\n",
    "  \"\"\"\n",
    "\n",
    "  best_aic = np.inf  # Initialize with positive infinity\n",
    "  best_bic = np.inf\n",
    "  best_model_aic = None\n",
    "  best_model_bic = None\n",
    "\n",
    "  for num_states in range(1, max_states + 1):\n",
    "    # Create initial distributions, transition matrix (random), and emission with Normal Distribution\n",
    "    initial_probs = Uniform(np.zeros(num_states), np.ones(num_states))\n",
    "    transition_matrix = np.random.rand(num_states, num_states)\n",
    "    transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)\n",
    "    emission_probs = Normal()  # N(0,1)\n",
    "\n",
    "    # Create the HMM model\n",
    "    model = DenseHMM()\n",
    "\n",
    "    # Fit the model to data\n",
    "    model.fit(data)\n",
    "\n",
    "    # Calculate AIC (same as before)\n",
    "    aic = 2 * model.num_params() - 2 * model.log_likelihood(data)\n",
    "\n",
    "    # Calculate BIC (same logic as before, but emission_params now considers mean and standard deviation)\n",
    "    emission_params = 2 * len(data[0])  # Mean and standard deviation for each emission dimension\n",
    "\n",
    "    # Number of parameters for transition matrix (excluding last row)\n",
    "    transition_params = num_states * (num_states - 1)\n",
    "\n",
    "    # Total number of free parameters\n",
    "    k = model.num_states - 1 + emission_params + transition_params\n",
    "\n",
    "    # Print statements for debugging (can be removed)\n",
    "    print(\"num params\", model.num_params())\n",
    "    print(\"k\", k)\n",
    "\n",
    "    bic = k * np.log(len(data)) - 2 * model.log_likelihood(data)\n",
    "\n",
    "    # Update best model if lower IC is found\n",
    "    if aic < best_aic:\n",
    "      best_aic = aic\n",
    "      best_model_aic = model\n",
    "    if bic < best_bic:\n",
    "      best_bic = bic\n",
    "      best_model_bic = model\n",
    "\n",
    "  return best_aic, best_model_aic, best_bic, best_model_bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter X must have 2 dims",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m----> 3\u001b[0m optimal_states, fitted_model \u001b[38;5;241m=\u001b[39m \u001b[43mselect_optimal_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal number of states (AIC): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_states\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitted model:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fitted_model)\n",
      "Cell \u001b[1;32mIn[45], line 29\u001b[0m, in \u001b[0;36mselect_optimal_states\u001b[1;34m(data, max_states)\u001b[0m\n\u001b[0;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m DenseHMM()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Fit the model to data\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Calculate AIC (same as before)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m aic \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mnum_params() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mlog_likelihood(data)\n",
      "File \u001b[1;32mc:\\Users\\alfredo.sampron\\AppData\\Local\\miniconda3\\envs\\hmm\\Lib\\site-packages\\pomegranate\\hmm\\_base.py:581\u001b[0m, in \u001b[0;36m_BaseHMM.fit\u001b[1;34m(self, X, sample_weight, priors)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, priors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    520\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to sequences with optional weights and priors.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03m\tThis method implements the core of the learning process. For hidden\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m\tself\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m \tX, sample_weight, priors \u001b[38;5;241m=\u001b[39m \u001b[43mpartition_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m \t\u001b[38;5;66;03m# Initialize by concatenating across sequences\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n",
      "File \u001b[1;32mc:\\Users\\alfredo.sampron\\AppData\\Local\\miniconda3\\envs\\hmm\\Lib\\site-packages\\pomegranate\\_utils.py:462\u001b[0m, in \u001b[0;36mpartition_sequences\u001b[1;34m(X, sample_weight, priors, n_dists)\u001b[0m\n\u001b[0;32m    459\u001b[0m priors_dict \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[1;32m--> 462\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[43m_check_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \tn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)\n\u001b[0;32m    465\u001b[0m \tX_dict[n]\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[1;32mc:\\Users\\alfredo.sampron\\AppData\\Local\\miniconda3\\envs\\hmm\\Lib\\site-packages\\pomegranate\\_utils.py:231\u001b[0m, in \u001b[0;36m_check_parameter\u001b[1;34m(parameter, name, min_value, max_value, value_sum, value_sum_dim, value_set, dtypes, ndim, shape, check_parameter, epsilon)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ndim, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    230\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parameter\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m ndim:\n\u001b[1;32m--> 231\u001b[0m \t\t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dims\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    232\u001b[0m \t\t\tname, ndim))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parameter\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ndim:\n",
      "\u001b[1;31mValueError\u001b[0m: Parameter X must have 2 dims"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "data = [[0, 1, 0, 1], [1, 0, 1, 0]]\n",
    "optimal_states, fitted_model = select_optimal_states(data)\n",
    "\n",
    "print(f\"Optimal number of states (AIC): {optimal_states}\")\n",
    "print(\"Fitted model:\", fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pomegranate' has no attribute 'distributions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pomegranate' has no attribute 'distributions'"
     ]
    }
   ],
   "source": [
    "pm.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"covariance_type\": \"diag\",\n",
    "    \"n_iter\": 500,\n",
    "    \"random_state\": random_state,\n",
    "    # no voy a usar startprob_prior por devlog 20-06-23\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hmm_model(\n",
    "    df: pd.DataFrame,\n",
    "    tickerlist: list,\n",
    "    range_states,\n",
    "    param_dict: dict,\n",
    "    contains_vol: bool,\n",
    "    contains_USD: bool,\n",
    "):\n",
    "\n",
    "    results_dict_df = {}\n",
    "\n",
    "    for stock in tickerlist:\n",
    "        results_dict_df[stock] = pd.DataFrame(\n",
    "            index=range_states, columns=[\"AIC\", \"BIC\"]\n",
    "        )\n",
    "        for nstate in range_states:\n",
    "            columns = generate_columns(stock, contains_vol, contains_USD)\n",
    "\n",
    "            insample_data = df[columns]\n",
    "\n",
    "            model = hmm.GaussianHMM(n_components=nstate, **param_dict, verbose=False)\n",
    "            results = model.fit(insample_data)\n",
    "\n",
    "            convergence = results.monitor_.converged\n",
    "            all_states_found = np.isclose(a=(model.transmat_.sum(axis=1)), b=1).all()\n",
    "            startprob_check = model.startprob_.sum() == 1\n",
    "            good_model = convergence and all_states_found and startprob_check\n",
    "\n",
    "            if good_model:\n",
    "                try:\n",
    "                    results_dict_df[stock].loc[nstate, \"AIC\"] = model.aic(insample_data)\n",
    "                    results_dict_df[stock].loc[nstate, \"BIC\"] = model.bic(insample_data)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                print(\">\" * 10, f\"{stock} {nstate} did not converge\")\n",
    "                results_dict_df[stock].loc[nstate, \"AIC\"] = np.inf\n",
    "                results_dict_df[stock].loc[nstate, \"BIC\"] = np.inf\n",
    "\n",
    "    return results_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> ^BVSP 12 did not converge\n",
      ">>>>>>>>>> ^BVSP 15 did not converge\n",
      ">>>>>>>>>> VALE3.SA 7 did not converge\n",
      ">>>>>>>>>> VALE3.SA 13 did not converge\n",
      ">>>>>>>>>> VALE3.SA 15 did not converge\n",
      ">>>>>>>>>> VALE 5 did not converge\n",
      ">>>>>>>>>> PETR3.SA 8 did not converge\n",
      ">>>>>>>>>> PETR3.SA 13 did not converge\n",
      ">>>>>>>>>> PBR 5 did not converge\n",
      ">>>>>>>>>> EMBR3.SA 11 did not converge\n",
      ">>>>>>>>>> ERJ 6 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 5 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 6 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 8 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 12 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 14 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 15 did not converge\n",
      ">>>>>>>>>> ABEV 5 did not converge\n",
      ">>>>>>>>>> ABEV 8 did not converge\n",
      ">>>>>>>>>> ABEV 10 did not converge\n",
      ">>>>>>>>>> ABEV 15 did not converge\n"
     ]
    }
   ],
   "source": [
    "results_dict_df_univ = fit_hmm_model(\n",
    "    df, tickerlist, range_states, param_dict, contains_vol=False, contains_USD=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> ^BVSP 5 did not converge\n",
      ">>>>>>>>>> VALE3.SA 3 did not converge\n",
      ">>>>>>>>>> VALE3.SA 7 did not converge\n",
      ">>>>>>>>>> VALE 5 did not converge\n",
      ">>>>>>>>>> VALE 7 did not converge\n",
      ">>>>>>>>>> VALE 15 did not converge\n",
      ">>>>>>>>>> PBR 3 did not converge\n",
      ">>>>>>>>>> PBR 13 did not converge\n",
      ">>>>>>>>>> PBR 15 did not converge\n",
      ">>>>>>>>>> EMBR3.SA 8 did not converge\n",
      ">>>>>>>>>> ERJ 2 did not converge\n",
      ">>>>>>>>>> ERJ 6 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 3 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 14 did not converge\n",
      ">>>>>>>>>> ABEV 2 did not converge\n",
      ">>>>>>>>>> ABEV 5 did not converge\n",
      ">>>>>>>>>> ABEV 15 did not converge\n"
     ]
    }
   ],
   "source": [
    "results_dict_df_with_vol = fit_hmm_model(\n",
    "    df, tickerlist, range_states, param_dict, contains_vol=True, contains_USD=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> ^BVSP 5 did not converge\n",
      ">>>>>>>>>> ^BVSP 14 did not converge\n",
      ">>>>>>>>>> VALE3.SA 2 did not converge\n",
      ">>>>>>>>>> VALE3.SA 5 did not converge\n",
      ">>>>>>>>>> VALE3.SA 6 did not converge\n",
      ">>>>>>>>>> VALE3.SA 7 did not converge\n",
      ">>>>>>>>>> VALE3.SA 13 did not converge\n",
      ">>>>>>>>>> VALE 12 did not converge\n",
      ">>>>>>>>>> VALE 14 did not converge\n",
      ">>>>>>>>>> VALE 15 did not converge\n",
      ">>>>>>>>>> PETR3.SA 5 did not converge\n",
      ">>>>>>>>>> PETR3.SA 6 did not converge\n",
      ">>>>>>>>>> PETR3.SA 14 did not converge\n",
      ">>>>>>>>>> PBR 5 did not converge\n",
      ">>>>>>>>>> PBR 14 did not converge\n",
      ">>>>>>>>>> PBR 15 did not converge\n",
      ">>>>>>>>>> EMBR3.SA 6 did not converge\n",
      ">>>>>>>>>> ERJ 3 did not converge\n",
      ">>>>>>>>>> ERJ 5 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 5 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 6 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 7 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 14 did not converge\n",
      ">>>>>>>>>> ABEV3.SA 15 did not converge\n",
      ">>>>>>>>>> ABEV 13 did not converge\n",
      ">>>>>>>>>> ABEV 14 did not converge\n"
     ]
    }
   ],
   "source": [
    "results_dict_df_multi = fit_hmm_model(\n",
    "    df, tickerlist, range_states, param_dict, contains_vol=True, contains_USD=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(\n",
    "    df: pd.DataFrame,\n",
    "    results_dict: dict,\n",
    "    tickerlist: list,\n",
    "    param_dict: dict,\n",
    "    contains_vol: bool,\n",
    "    contains_USD: bool,\n",
    "):\n",
    "    \"\"\"\"\"\"\n",
    "    aic_best_model = {stock: None for stock in tickerlist}\n",
    "    bic_best_model = {stock: None for stock in tickerlist}\n",
    "\n",
    "    for stock in tickerlist:\n",
    "        columns = generate_columns(stock, contains_vol, contains_USD)\n",
    "        insample_data = df[columns]\n",
    "\n",
    "        best_aic_nstate = results_dict[stock][\"AIC\"].astype(float).idxmin()\n",
    "        best_bic_nstate = results_dict[stock][\"BIC\"].astype(float).idxmin()\n",
    "\n",
    "        print(\n",
    "            f\"For stock {stock}, best AIC: {best_aic_nstate} best BIC: {best_bic_nstate}\"\n",
    "        )\n",
    "\n",
    "        aic_best_model[stock] = hmm.GaussianHMM(\n",
    "            n_components=best_aic_nstate, **param_dict\n",
    "        ).fit(insample_data)\n",
    "\n",
    "        bic_best_model[stock] = hmm.GaussianHMM(\n",
    "            n_components=best_bic_nstate, **param_dict\n",
    "        ).fit(insample_data)\n",
    "\n",
    "    return aic_best_model, bic_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For stock ^BVSP, best AIC: 6 best BIC: 2\n",
      "For stock VALE3.SA, best AIC: 5 best BIC: 2\n",
      "For stock VALE, best AIC: 4 best BIC: 2\n",
      "For stock PETR3.SA, best AIC: 4 best BIC: 4\n",
      "For stock PBR, best AIC: 4 best BIC: 3\n",
      "For stock EMBR3.SA, best AIC: 4 best BIC: 4\n",
      "For stock ERJ, best AIC: 4 best BIC: 4\n",
      "For stock ABEV3.SA, best AIC: 4 best BIC: 2\n",
      "For stock ABEV, best AIC: 4 best BIC: 2\n"
     ]
    }
   ],
   "source": [
    "aic_best_model_univ, bic_best_model_univ = select_best_model(\n",
    "    df=df,\n",
    "    results_dict=results_dict_df_univ,\n",
    "    tickerlist=tickerlist,\n",
    "    param_dict=param_dict,\n",
    "    contains_vol=False,\n",
    "    contains_USD=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For stock ^BVSP, best AIC: 2 best BIC: 2\n",
      "For stock VALE3.SA, best AIC: 2 best BIC: 2\n",
      "For stock VALE, best AIC: 2 best BIC: 2\n",
      "For stock PETR3.SA, best AIC: 2 best BIC: 2\n",
      "For stock PBR, best AIC: 4 best BIC: 2\n",
      "For stock EMBR3.SA, best AIC: 3 best BIC: 2\n",
      "For stock ERJ, best AIC: 4 best BIC: 4\n",
      "For stock ABEV3.SA, best AIC: 2 best BIC: 2\n",
      "For stock ABEV, best AIC: 4 best BIC: 4\n"
     ]
    }
   ],
   "source": [
    "aic_best_model_with_vol, bic_best_model_with_vol = select_best_model(\n",
    "    df=df,\n",
    "    results_dict=results_dict_df_with_vol,\n",
    "    tickerlist=tickerlist,\n",
    "    param_dict=param_dict,\n",
    "    contains_vol=False,\n",
    "    contains_USD=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For stock ^BVSP, best AIC: 2 best BIC: 2\n",
      "For stock VALE3.SA, best AIC: 3 best BIC: 3\n",
      "For stock VALE, best AIC: 2 best BIC: 2\n",
      "For stock PETR3.SA, best AIC: 2 best BIC: 2\n",
      "For stock PBR, best AIC: 2 best BIC: 2\n",
      "For stock EMBR3.SA, best AIC: 2 best BIC: 2\n",
      "For stock ERJ, best AIC: 2 best BIC: 2\n",
      "For stock ABEV3.SA, best AIC: 2 best BIC: 2\n",
      "For stock ABEV, best AIC: 2 best BIC: 2\n"
     ]
    }
   ],
   "source": [
    "aic_best_model_multi, bic_best_model_multi = select_best_model(\n",
    "    df=df,\n",
    "    results_dict=results_dict_df_multi,\n",
    "    tickerlist=tickerlist,\n",
    "    param_dict=param_dict,\n",
    "    contains_vol=False,\n",
    "    contains_USD=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating out of sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'finaldf_test_{params[\"tablename\"]}.pickle'\n",
    "filename = os.path.join(dataroute, name)\n",
    "with open(filename, \"rb\") as handle:\n",
    "    df_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_residuals(actual: pd.DataFrame, forecasts: pd.DataFrame):\n",
    "    residuals = (actual - forecasts)\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_HMM_samples_residuals(model, insample_data, oos_data):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        insample_data (_type_): _description_\n",
    "        oos_data (_type_): _description_\n",
    "    \"\"\"\n",
    "    # pseudocodigo\n",
    "    # agarra el mejor modelo (esto con una cantidad optima de params ya esta)\n",
    "    # fittear t-j con t-j-252d\n",
    "    # Darle un aÃ±o de datos hasta t-j para que me prediga la secuencia (probabilidad) de estados.\n",
    "    # Le pido que me prediga las probabilidades de cada estado durante el periodo t-j, t-j-252:\n",
    "    # esto me da una matriz de (252 x n estados)\n",
    "    # esto entiendo es https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GaussianHMM.predict_proba\n",
    "    # Tomo la ultima fila de la matriz\n",
    "    # Multiplico esa por el vector de medias estimadas: este punto es mi forecast.\n",
    "    # esto es model.means_ (!)\n",
    "    nstate = model.n_components\n",
    "    columns = oos_data.columns\n",
    "\n",
    "    split_date = oos_data.index[0]\n",
    "    dates_to_forecast = len(oos_data.index)\n",
    "\n",
    "    probabilities = pd.DataFrame(columns=range(nstate), index=oos_data.index)\n",
    "    forecasts = pd.DataFrame(columns=oos_data.columns, index=oos_data.index)\n",
    "\n",
    "    full_data = pd.concat([insample_data, oos_data])\n",
    "    del insample_data\n",
    "\n",
    "    # vamos a implementar recursive window forecasting\n",
    "\n",
    "    index = full_data.index\n",
    "    end_loc = np.where(index >= split_date)[0].min()\n",
    "    # esto es un int del iloc\n",
    "    # preciso usar ints de iloc porque el timedelta se me va a romper con el fin de semana\n",
    "    rolling_window = 252\n",
    "\n",
    "    nstate = model.n_components\n",
    "    model = hmm.GaussianHMM(n_components=nstate, **param_dict, verbose=False)\n",
    "\n",
    "    model_list = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(1, dates_to_forecast):\n",
    "        date_of_first_forecast = full_data.index[end_loc + i - 1]\n",
    "\n",
    "        fitstart = end_loc - rolling_window + i\n",
    "        fitend = end_loc + i\n",
    "\n",
    "        # fit model with last year\n",
    "        fit_data = full_data.iloc[fitstart:fitend][columns]\n",
    "        res = model.fit(fit_data)\n",
    "        model_list.append(res)\n",
    "\n",
    "        # obtenemos las probabilidades por estado del ultimo dia\n",
    "        # son las probabilidades que maximizan la log/likelihood de toda la secuencia\n",
    "        index = len(model_list)\n",
    "        while index > 0:\n",
    "            try:\n",
    "                add_count = False\n",
    "                last_day_state_probs = res.predict_proba(fit_data)[-1]\n",
    "                probabilities.loc[date_of_first_forecast] = last_day_state_probs\n",
    "                index = 0\n",
    "\n",
    "            except ValueError:\n",
    "                # this happens when startprob_ must sum to 1 (got nan)\n",
    "                # si el modelo falla en el predict_proba, se utiliza el de t-1\n",
    "                add_count = True\n",
    "                index = index - 1\n",
    "                res = model_list[index]\n",
    "\n",
    "                if not \"last_day_state_probs\" in locals():\n",
    "                    # this checks for failure of estimation in the first day\n",
    "                    last_day_state_probs = np.full(nstate, (1 / nstate))\n",
    "                    # inputs a flat prior if it has no previous day to fall back on\n",
    "\n",
    "        if add_count:\n",
    "            counter = counter + 1\n",
    "        # model.means_ es es la media condicional a cada estado\n",
    "        # cada columna representa cada columna del dataset\n",
    "        # cada fila es un estado\n",
    "        # el producto punto entre este y las probabilidades del ultimo dÃ­a me da la media esperada por cada columna\n",
    "        expected_means = np.dot(last_day_state_probs, model.means_)\n",
    "        forecasts.loc[date_of_first_forecast] = expected_means\n",
    "\n",
    "    pct_nan = forecasts.iloc[:, 0].isna().sum() / len(forecasts.index) * 100\n",
    "\n",
    "    if pct_nan > 5:\n",
    "        warnings.warn(f\"{oos_data.columns[0]} % na: {pct_nan}\")\n",
    "\n",
    "    forecasts.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "    residuals = return_residuals(oos_data, forecasts)\n",
    "\n",
    "    print(\"failed models: \", counter)\n",
    "    return probabilities, forecasts, residuals, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_samples(\n",
    "    best_model_dict: dict,\n",
    "    modeltype: str,\n",
    "    criterion: str,\n",
    "    insample_data: pd.DataFrame,\n",
    "    oos_data: pd.DataFrame,\n",
    "    tickerlist: list,\n",
    "    contains_vol: bool,\n",
    "    contains_USD: bool,\n",
    "):\n",
    "    probabilities = {stock: None for stock in tickerlist}\n",
    "    forecasts = {stock: None for stock in tickerlist}\n",
    "    residuals = {stock: None for stock in tickerlist}\n",
    "    failed = {stock: None for stock in tickerlist}\n",
    "\n",
    "    print(\">\" * 10, modeltype, criterion)\n",
    "\n",
    "    for stock in tickerlist:\n",
    "        print(stock)\n",
    "        columns = generate_columns(\n",
    "            stock=stock, contains_vol=contains_vol, contains_USD=contains_USD\n",
    "        )\n",
    "\n",
    "        proba, fcast, resid, fails = generate_HMM_samples_residuals(\n",
    "            best_model_dict[stock],\n",
    "            insample_data=insample_data[columns],\n",
    "            oos_data=oos_data[columns],\n",
    "        )\n",
    "\n",
    "        probabilities[stock] = proba\n",
    "        forecasts[stock] = fcast\n",
    "        residuals[stock] = resid\n",
    "        failed[stock] = fails\n",
    "\n",
    "    save_as_pickle(\n",
    "        data=forecasts,\n",
    "        resultsroute=params[\"resultsroute\"],\n",
    "        model_type=f\"HMM_{modeltype}\",\n",
    "        tablename=params[\"tablename\"],\n",
    "        criterion=criterion,\n",
    "        type_save=\"forecasts\",\n",
    "    )\n",
    "\n",
    "    save_as_pickle(\n",
    "        data=residuals,\n",
    "        resultsroute=params[\"resultsroute\"],\n",
    "        model_type=f\"HMM_{modeltype}\",\n",
    "        tablename=params[\"tablename\"],\n",
    "        criterion=criterion,\n",
    "        type_save=\"residuals\",\n",
    "    )\n",
    "\n",
    "    save_as_pickle(\n",
    "        data=failed,\n",
    "        resultsroute=params[\"resultsroute\"],\n",
    "        model_type=f\"HMM_{modeltype}\",\n",
    "        tablename=params[\"tablename\"],\n",
    "        criterion=criterion,\n",
    "        type_save=\"model_fails\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"aic\": {\n",
    "        \"univ\": (aic_best_model_univ, False, False),\n",
    "        \"with_vol\": (aic_best_model_with_vol, True, False),\n",
    "        \"multiv\": (aic_best_model_multi, True, True),\n",
    "    },\n",
    "    \"bic\": {\n",
    "        \"univ\": (bic_best_model_univ, False, False),\n",
    "        \"with_vol\": (bic_best_model_with_vol, True, False),\n",
    "        \"multiv\": (bic_best_model_multi, True, True),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> univ aic\n",
      "^BVSP\n",
      "failed models:  0\n",
      "VALE3.SA\n",
      "failed models:  0\n",
      "VALE\n",
      "failed models:  0\n",
      "PETR3.SA\n",
      "failed models:  0\n",
      "PBR\n",
      "failed models:  0\n",
      "EMBR3.SA\n",
      "failed models:  0\n",
      "ERJ\n",
      "failed models:  0\n",
      "ABEV3.SA\n",
      "failed models:  0\n",
      "ABEV\n",
      "failed models:  0\n",
      ">>>>>>>>>> with_vol aic\n",
      "^BVSP\n",
      "failed models:  0\n",
      "VALE3.SA\n",
      "failed models:  0\n",
      "VALE\n",
      "failed models:  0\n",
      "PETR3.SA\n",
      "failed models:  0\n",
      "PBR\n",
      "failed models:  0\n",
      "EMBR3.SA\n",
      "failed models:  0\n",
      "ERJ\n",
      "failed models:  0\n",
      "ABEV3.SA\n",
      "failed models:  0\n",
      "ABEV\n",
      "failed models:  0\n",
      ">>>>>>>>>> multiv aic\n",
      "^BVSP\n",
      "failed models:  0\n",
      "VALE3.SA\n",
      "failed models:  3\n",
      "VALE\n",
      "failed models:  0\n",
      "PETR3.SA\n",
      "failed models:  0\n",
      "PBR\n",
      "failed models:  0\n",
      "EMBR3.SA\n",
      "failed models:  0\n",
      "ERJ\n",
      "failed models:  0\n",
      "ABEV3.SA\n",
      "failed models:  0\n",
      "ABEV\n",
      "failed models:  0\n",
      ">>>>>>>>>> univ bic\n",
      "^BVSP\n",
      "failed models:  0\n",
      "VALE3.SA\n",
      "failed models:  0\n",
      "VALE\n",
      "failed models:  0\n",
      "PETR3.SA\n",
      "failed models:  0\n",
      "PBR\n",
      "failed models:  0\n",
      "EMBR3.SA\n",
      "failed models:  0\n",
      "ERJ\n",
      "failed models:  0\n",
      "ABEV3.SA\n",
      "failed models:  0\n",
      "ABEV\n",
      "failed models:  0\n",
      ">>>>>>>>>> with_vol bic\n",
      "^BVSP\n",
      "failed models:  0\n",
      "VALE3.SA\n",
      "failed models:  0\n",
      "VALE\n",
      "failed models:  0\n",
      "PETR3.SA\n",
      "failed models:  0\n",
      "PBR\n",
      "failed models:  0\n",
      "EMBR3.SA\n",
      "failed models:  0\n",
      "ERJ\n",
      "failed models:  0\n",
      "ABEV3.SA\n",
      "failed models:  0\n",
      "ABEV\n",
      "failed models:  0\n",
      ">>>>>>>>>> multiv bic\n",
      "^BVSP\n",
      "failed models:  0\n",
      "VALE3.SA\n",
      "failed models:  3\n",
      "VALE\n",
      "failed models:  0\n",
      "PETR3.SA\n",
      "failed models:  0\n",
      "PBR\n",
      "failed models:  0\n",
      "EMBR3.SA\n",
      "failed models:  0\n",
      "ERJ\n",
      "failed models:  0\n",
      "ABEV3.SA\n",
      "failed models:  0\n",
      "ABEV\n",
      "failed models:  0\n"
     ]
    }
   ],
   "source": [
    "for criterion, type_dict in models_dict.items():\n",
    "    for modeltype, tupla in type_dict.items():\n",
    "        best_dict, contains_vol, contains_USD = tupla\n",
    "        try:\n",
    "            generate_and_save_samples(\n",
    "                best_model_dict=best_dict,\n",
    "                modeltype=modeltype,\n",
    "                criterion=criterion,\n",
    "                insample_data=df,\n",
    "                oos_data=df_test,\n",
    "                tickerlist=params[\"tickerlist\"],\n",
    "                contains_vol=contains_vol,\n",
    "                contains_USD=contains_USD,\n",
    "            )\n",
    "        except UnboundLocalError:\n",
    "            print(f\"MODEL FALILURE: {criterion}, {modeltype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=f\"\"\"HMM_multiv_{params[\"tablename\"]}_aic_best_residuals.pickle\"\"\"\n",
    "with open(os.path.join(resultsroute, file), \"rb\") as f:\n",
    "    opened_pickle=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^BVSP_log_rets</th>\n",
       "      <th>^BVSP_gk_vol</th>\n",
       "      <th>USD_log_rets</th>\n",
       "      <th>USD_gk_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-01</th>\n",
       "      <td>0.005412</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.007433</td>\n",
       "      <td>-0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-04</th>\n",
       "      <td>-0.010738</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.012951</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-05</th>\n",
       "      <td>0.000191</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>-0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-06</th>\n",
       "      <td>-0.010660</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.004736</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-07</th>\n",
       "      <td>0.002553</td>\n",
       "      <td>-0.000152</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ^BVSP_log_rets  ^BVSP_gk_vol  USD_log_rets  USD_gk_vol\n",
       "2023-12-01        0.005412     -0.000107     -0.007433   -0.000024\n",
       "2023-12-04       -0.010738     -0.000073      0.012951    0.000010\n",
       "2023-12-05        0.000191     -0.000159     -0.002534   -0.000028\n",
       "2023-12-06       -0.010660     -0.000033     -0.004736   -0.000008\n",
       "2023-12-07        0.002553     -0.000152      0.000697    0.000005"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opened_pickle[params[\"index\"]].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HMM_multiv_BR_^BVSP_aic_best_model_fails.pickle': '..\\\\results\\\\BR_^BVSP\\\\HMM_multiv_BR_^BVSP_aic_best_model_fails.pickle', 'HMM_multiv_BR_^BVSP_bic_best_model_fails.pickle': '..\\\\results\\\\BR_^BVSP\\\\HMM_multiv_BR_^BVSP_bic_best_model_fails.pickle', 'HMM_univ_BR_^BVSP_aic_best_model_fails.pickle': '..\\\\results\\\\BR_^BVSP\\\\HMM_univ_BR_^BVSP_aic_best_model_fails.pickle', 'HMM_univ_BR_^BVSP_bic_best_model_fails.pickle': '..\\\\results\\\\BR_^BVSP\\\\HMM_univ_BR_^BVSP_bic_best_model_fails.pickle', 'HMM_with_vol_BR_^BVSP_aic_best_model_fails.pickle': '..\\\\results\\\\BR_^BVSP\\\\HMM_with_vol_BR_^BVSP_aic_best_model_fails.pickle', 'HMM_with_vol_BR_^BVSP_bic_best_model_fails.pickle': '..\\\\results\\\\BR_^BVSP\\\\HMM_with_vol_BR_^BVSP_bic_best_model_fails.pickle'}\n"
     ]
    }
   ],
   "source": [
    "fails_dict=get_all_results_matching(resultsroute, [\"fail\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails_df=pd.DataFrame()\n",
    "for name, dir in fails_dict.items():\n",
    "    dict_with_dfs = pd.read_pickle(dir)\n",
    "    colname = clean_modelname(name, substring_to_replace=\"model_fails\", tablename=params[\"tablename\"])\n",
    "    fails_df[colname]=dict_with_dfs\n",
    "    os.remove(dir)\n",
    "\n",
    "fails_df=fails_df/len(df_test.index)\n",
    "fails_df.to_csv(path_or_buf=os.path.join(params[\"resultsroute\"], f\"\"\"HMM_{params[\"tablename\"]}_fails.csv\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_close_rets_vol(model, data, key, IC):\n",
    "    prediction = model.predict(data)\n",
    "    states = set(prediction)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    plt.tight_layout()\n",
    "    plt.title(\n",
    "        f\"{key} Log returns and intraday Vol\\n{model.n_components} states / best by {IC}\"\n",
    "    )\n",
    "\n",
    "    for subplot, var in zip(range(1, 3), data.columns):\n",
    "        plt.subplot(2, 1, subplot)\n",
    "        for i in set(prediction):\n",
    "            state = prediction == i\n",
    "            x = data.index[state]\n",
    "            y = data[var].iloc[state]\n",
    "            plt.plot(x, y, \".\")\n",
    "        plt.legend(states, fontsize=16)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"datetime\", fontsize=16)\n",
    "        plt.ylabel(var, fontsize=16)\n",
    "\n",
    "    plt.savefig(os.path.join(resultsroute, \"graphs\", f\"HMM\", f\"{key}_model_{IC}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for dictionary, IC in zip([aic_best_model, bic_best_model], [\"AIC\", \"BIC\"]):\n",
    "#    for key, model in dictionary.items():\n",
    "#        columns = [f\"{stock}_log_rets\", f\"{stock}_gk_vol\"]\n",
    "#        insample_data = df[columns]\n",
    "#        oos_data = df_test[columns]\n",
    "#        train_end = insample_data.index.max()\n",
    "#        data = pd.concat([insample_data, oos_data])\n",
    "#\n",
    "#        plot_close_rets_vol(model, data, key, IC)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
